{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exercise 1\n",
    "## Implement Q learning in the cartpole environment\n",
    "\n",
    "\n",
    "Reinforcement learning is trying to optimise behaviour based on rewards.\n",
    "Using games as environments is easy and fast to repeat, which makes it ideal for learning by trial and error\n",
    "\n",
    "\n",
    "### Markov decision processes\n",
    "Decisionmaker = Agent\n",
    "In this example, the agent is the cart, and its actions are moving either left or right\n",
    "\n",
    "Environment -> Agent -> States -> Actions -> Rewars -> Repeat\n",
    "\n",
    "The agent wants to achieve higher cumulative reward, not just instant rewards.\n",
    "\n",
    "Set of states = S\n",
    "Set of actions = A\n",
    "Set of rewards = R\n",
    "\n",
    "At each time step (t = 0,1,2...) the agent receives a set of the environments state St.\n",
    "Based on this state, the agent selects and action At. This gives a set pair og action and state (St, At).\n",
    "Time is then incremented t + 1, and the environment is updated with new state. S(t + 1).\n",
    "At this time, the agent receives a reward R(t + 1), taken from the action At. The reward R(t + 1) is based on the state-action pair(St, At)\n",
    "\n",
    "We can look at this as a function *f*(St, At) = R(t +1)\n",
    "\n",
    "This is a sequential process, which can be presented like this: S0, A0, R1, S1, A1, R2, S2...\n",
    "\n",
    "![Illustrated diagram](images/Environment-state-action-flow.png)\n",
    "\n",
    "### Expected return\n",
    "The goal for the agent is to maximise the cumulative rewards. The return is the sum of future rewards.\n",
    "Gt = Rt+1 + Rt+2 + Rt+3 + ... + RT\n",
    "\n",
    "T is the final time step.\n",
    "\n",
    "The interactions of the agent with the actions and environment breaks up into episodes. Where a rewards is calculated Rt+1 at the end of every episode.\n",
    "The environment is reset and the agent can start over with new state.\n",
    "\n",
    "We modify the agent to try to maximise the cumulative discounted rewards.\n",
    "\n",
    "The discount rate d(gamma) = a number between 0 and 1\n",
    "\n",
    "The discounted reward will be Gt = Rt+1 + d²(Rt+2) + d³(Rt+3) + d⁴(Rt+4), Gt is the sum of the discounted rewards at each timestep\n",
    "\n",
    "This will lead the agent to prioritise current rewards, since future rewards will be more discounted.\n",
    "\n",
    "### Policies and value functions\n",
    "How likely is an agent to take any given action based on the state?\n",
    "\n",
    "#### Policies\n",
    "A policy is a function which maps a given state which to the probability of selecting each possible action from that state.\n",
    "Generally an agent follows a policy. If an agent follows policy p at a time t, then p(a | s) is the probability that At = a if St = s.\n",
    "This means that, at time t, under policy p, the probability of taking action a in state s is p(a|s)\n",
    "\n",
    "#### Value functions\n",
    "Value functions determine how good it is for an agent to perform a given action in a given state.\n",
    "The value the value function return is the Expected return.\n",
    "\n",
    "We have a state-value function and the action-value function\n",
    "\n",
    "#### state-value function\n",
    "The state-value function for policy p denoted as q<sub>p</sub> tells us how good it is for the agent to take any given action from a given state while following policy p.\n",
    "\n",
    "In other words, the Q function gives us the value of an action under policy p\n",
    "\n",
    "q<sub>p</sub>(s, a )\n",
    "\n",
    "\n",
    "### Optimal policies\n",
    "The goal for the agent is to find the optimal policy which will yield the highest rewards.\n",
    "A new policy is generally thought of as better if the policy yields a higher or the same award as another policy for all states.\n",
    "The best policy, is called the optimal policy. The agent's goal is to find this policy.\n",
    "\n",
    "#### Optimal state-value function\n",
    "The optimal policy has an associated state-value function.\n",
    "\n",
    "#### Optimal action-value function\n",
    "The optimal policy also has an optimal action-value function. This is called the Optimal q function q*.\n",
    "q* is the highest possible reward possible for each state-action pair. The agent wil try to find a policy that converges with q*\n",
    "q* must satisfy the Bellman optimality equation.\n",
    "\n",
    "We use the Bellman optimality equation to find q*, which we can use to find the action a which will produce the best Q value for the next state\n",
    "\n",
    "\n",
    "### Q - learning\n",
    "The agent tries to learn the best policy by learning the best q-values for each state-action pair.\n",
    "The agent will update the Q table for any given state-action pair until the q-function converges to the optimal q function q*.\n",
    "This is called value iteration.\n",
    "\n",
    "The cartpole can analyse the state, position, velocity and angle and determine the optimal q value for every action(left or right)\n",
    "\n",
    "### Exploration vs explotation\n",
    "To force our agent to learn new things about the environment. This is a strategy called an Epslion-greedy strategy\n",
    "We force our agent to explore more in the start and gradually start exploiting it's findings for better rewards.\n",
    "\n",
    "\n",
    "### Learning rate\n",
    "The learning rate signifies how quickly the agent will switch out a Q value for another q value.\n",
    "The learning rate is a tool to determine how much data we will store from the last q value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import math\n",
    "\n",
    "cartpole_environment = gym.make(\"CartPole-v1\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "#Constants\n",
    "LR = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 20000\n",
    "\n",
    "#Variables\n",
    "total = 0\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "\n",
    "# Cart position, cart velocity, pole angle, pole velocity\n",
    "# Number of different values in every bucket\n",
    "# The number of buckets in each does not seem to matter that much, I tried 100 in each\n",
    "observation = [30, 30, 50, 50]\n",
    "\n",
    "#Steps\n",
    "#Cart position, cart velocity, ple angle, pole velocity\n",
    "scaling_values = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "\n",
    "\n",
    "\n",
    "# Exploration rate, just called epsilon\n",
    "exploration_rate = 1\n",
    "exploration_rate_decay = 0.99995\n",
    "exploration_rate_minimum_threshold = 0.05\n",
    "\n",
    "\n",
    "# The Q table is a policy table, which will be used by the agent to determine the next move\n",
    "# Every move will be determined as positive or negative\n",
    "# The Q table starts with zeroes, which will be optimise as the Q table gets better\n",
    "#q_table = np.zeros(observation + [cartpole_environment.action_space.n])\n",
    "\n",
    "# The Q table starts with randomized values, which will be optimised as the exploration rate decreases\n",
    "q_table = np.random.uniform(low=0, high=1, size=(observation + [cartpole_environment.action_space.n]))\n",
    "\n",
    "# Q-table shape: [30, 30, 50, 50, 2]\n",
    "\n",
    "\n",
    "#Getting the discrete state is dealing with the problems of continuos state. This function will group similiar state values into \"buckets\".\n",
    "#This yields a more manageable state-space, which we can use to calculate values\n",
    "# The touple returned from this function is a reduced discretisised state we can use to make calculations\n",
    "# This function takes state/observation and converts it into values we can evaluate with a Q function and update new state\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = state/scaling_values + np.array([15, 10, 1, 10])\n",
    "    return tuple(discrete_state.astype(np.int))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7045/3844189879.py:45: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return tuple(discrete_state.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Time Average: 0.0005083670616149903\n",
      "Mean Reward: 0.025\n",
      "Episode: 1000\n",
      "Time Average: 0.001532392978668213\n",
      "Mean Reward: 22.643\n",
      "Episode: 2000\n",
      "Time Average: 0.0007491934299468994\n",
      "Mean Reward: 22.256\n",
      "Episode: 3000\n",
      "Time Average: 0.0007159838676452637\n",
      "Mean Reward: 22.598\n",
      "Episode: 4000\n",
      "Time Average: 0.000649308204650879\n",
      "Mean Reward: 22.028\n",
      "Episode: 5000\n",
      "Time Average: 0.000882774829864502\n",
      "Mean Reward: 22.644\n",
      "Episode: 6000\n",
      "Time Average: 0.0007830579280853271\n",
      "Mean Reward: 22.396\n",
      "Episode: 7000\n",
      "Time Average: 0.0007486982345581055\n",
      "Mean Reward: 22.122\n",
      "Episode: 8000\n",
      "Time Average: 0.0006658680438995361\n",
      "Mean Reward: 21.671\n",
      "Episode: 9000\n",
      "Time Average: 0.0008325941562652587\n",
      "Mean Reward: 22.114\n",
      "Episode: 10000\n",
      "Time Average: 0.0010823590755462647\n",
      "Mean Reward: 22.122\n",
      "Exploration rate: 0.6218776713776856\n",
      "Episode: 11000\n",
      "Time Average: 0.00103249192237854\n",
      "Mean Reward: 36.1\n",
      "Exploration rate: 0.5915475999948323\n",
      "Episode: 12000\n",
      "Time Average: 0.0011491799354553223\n",
      "Mean Reward: 38.721\n",
      "Episode: 13000\n",
      "Exploration rate: 0.5488034037068503\n",
      "Time Average: 0.0016827876567840575\n",
      "Mean Reward: 40.187\n",
      "Episode: 14000\n",
      "Exploration rate: 0.5220372933033263\n",
      "Time Average: 0.0025329494476318357\n",
      "Mean Reward: 46.911\n",
      "Exploration rate: 0.5091478283790776\n",
      "Episode: 15000\n",
      "Exploration rate: 0.4965766133349901\n",
      "Time Average: 0.001765937089920044\n",
      "Mean Reward: 49.48\n",
      "Exploration rate: 0.484315790359524\n",
      "Episode: 16000\n",
      "Exploration rate: 0.47235769565598784\n",
      "Time Average: 0.0028992395401000975\n",
      "Mean Reward: 55.893\n",
      "Episode: 17000\n",
      "Exploration rate: 0.44931997732828616\n",
      "Time Average: 0.0023159263134002687\n",
      "Mean Reward: 60.769\n",
      "Episode: 18000\n",
      "Exploration rate: 0.4274058491752072\n",
      "Time Average: 0.002649301290512085\n",
      "Mean Reward: 66.117\n",
      "Episode: 19000\n",
      "Time Average: 0.0025155973434448243\n",
      "Mean Reward: 68.877\n",
      "Exploration rate: 0.396522249086328\n",
      "Episode: 20000\n",
      "Exploration rate: 0.3867318381581326\n",
      "Time Average: 0.003049398422241211\n",
      "Mean Reward: 72.015\n"
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODES + 1): # Adding +1 So it will complete the epochs on the final number\n",
    "    time_0 = time.time() # t0 for timing when we started balancing\n",
    "\n",
    "    discrete_state = get_discrete_state(cartpole_environment.reset())\n",
    "\n",
    "    done = False\n",
    "\n",
    "    episode_reward = 0 # Initialising reward for this episode\n",
    "\n",
    "    if episode % 1000 == 0: # Just printing the Episode\n",
    "        print(\"Episode: \" + str(episode))\n",
    "\n",
    "    while not done: # continue balancing the pole as long as it has not fallen\n",
    "\n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            # Choosing an exploitation action\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else: # Choosing a random action from the environment\n",
    "            action = cartpole_environment.action_space.sample()\n",
    "\n",
    "        # Passing our action the to the environment\n",
    "        # New state is the new state we have to work with/ Also called observation\n",
    "        # Reward is the reward for the action we chose\n",
    "        # done signals if the action led to failure, which will close this episode\n",
    "        # info contains diagnostics, which are not used at the moment, could be _\n",
    "        new_state, reward, done, info = cartpole_environment.step(action)\n",
    "\n",
    "        # Updates the reward for the current episode\n",
    "        episode_reward += reward\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "\n",
    "        #Rendering the gui showing the crazy moves of the agent\n",
    "        if episode % 1000 == 0:\n",
    "            # The cartpole will simply freeze when the episode ends, and will wait for the next 1000 iterations\n",
    "            cartpole_environment.render()\n",
    "\n",
    "\n",
    "        if not done:\n",
    "            # What is the highest possible q value?\n",
    "            max_q = np.max(q_table[new_discrete_state])\n",
    "            # Current q value\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            new_q = current_q * (1 - LR) + LR * (reward + DISCOUNT * max_q)\n",
    "\n",
    "            # Updating our current q_table with new state and action\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        # Discrete state is updated\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    # Checking if the exploration rate is greater than the threshold\n",
    "    if exploration_rate > exploration_rate_minimum_threshold:\n",
    "        # Reducing the exploration rate\n",
    "        if episode_reward > prior_reward and episode > 10000:\n",
    "            exploration_rate = math.pow(exploration_rate_decay, episode - 1000)\n",
    "\n",
    "            if episode % 500 == 0:\n",
    "                print(\"Exploration rate: \" + str(exploration_rate))\n",
    "\n",
    "    # Measuring the time spent balancing\n",
    "    time_1 = time.time()\n",
    "    episode_total = time_1 - time_0\n",
    "\n",
    "    #Updating the total\n",
    "    total = total + episode_total\n",
    "\n",
    "    #Updating the total reward\n",
    "    total_reward += episode_reward\n",
    "\n",
    "    # Saving the last reward\n",
    "    prior_reward = episode_reward\n",
    "\n",
    "    #Measuring averages\n",
    "    if episode % 1000 == 0:\n",
    "        mean = total / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "\n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "\n",
    "cartpole_environment.close()\n",
    "\n",
    "\n",
    "\n",
    "#Mean reward after 20000 episodes == 77 with buckets [30, 30, 50, 50]\n",
    "#2000 episodes, mean reward = 79, time average = 0.00726 with buckets [50, 50, 80, 80]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}