{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "char_encodings = [\n",
    "    [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # ' '\n",
    "    [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'h'\n",
    "    [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'a'\n",
    "    [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 't'\n",
    "    [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'r'\n",
    "    [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],  # 'c'\n",
    "    [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],  # 'f'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],  # 'l'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],  # 'm'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],  # 'p'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],  # 's'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],  # 'o'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]   # 'n'\n",
    "]\n",
    "char_encoding_size = len(char_encodings)\n",
    "\n",
    "index_to_char = [' ', 'h', 'a', 't', 'r', 'c', 'f', 'l', 'm', 'p', 's', 'o', 'n']\n",
    "\n",
    "hat = [[char_encodings[1]], [char_encodings[2]], [char_encodings[3]], [char_encodings[0]]]\n",
    "rat = [[char_encodings[4]], [char_encodings[2]], [char_encodings[3]], [char_encodings[0]]]\n",
    "cat = [[char_encodings[5]], [char_encodings[2]], [char_encodings[3]], [char_encodings[0]]]\n",
    "flat = [[char_encodings[6]], [char_encodings[7]], [char_encodings[2]], [char_encodings[3]]]\n",
    "matt = [[char_encodings[8]], [char_encodings[2]], [char_encodings[3]], [char_encodings[3]]]\n",
    "cap =  [[char_encodings[5]], [char_encodings[2]], [char_encodings[9]], [char_encodings[0]]]\n",
    "son =  [[char_encodings[10]], [char_encodings[11]], [char_encodings[12]], [char_encodings[0]]]\n",
    "\n",
    "hat_y = [char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[0]]\n",
    "rat_y = [char_encodings[4], char_encodings[2], char_encodings[3], char_encodings[0]]\n",
    "cat_y = [char_encodings[5], char_encodings[2], char_encodings[3], char_encodings[0]]\n",
    "flat_y = [char_encodings[6], char_encodings[7], char_encodings[2], char_encodings[3]]\n",
    "matt_y = [char_encodings[8], char_encodings[2], char_encodings[3], char_encodings[3]]\n",
    "cap_y =  [char_encodings[5], char_encodings[2], char_encodings[9], char_encodings[0]]\n",
    "son_y =  [char_encodings[10], char_encodings[11], char_encodings[12], char_encodings[0]]\n",
    "\n",
    "emojis = {\n",
    "        'hat': '\\U0001F3A9',\n",
    "        'cat': '\\U0001F408',\n",
    "        'rat': '\\U0001F400',\n",
    "        'flat': '\\U0001F3E2',\n",
    "        'matt': '\\U0001F468',\n",
    "        'cap': '\\U0001F9E2',\n",
    "        'son': '\\U0001F466'\n",
    "    }\n",
    "\n",
    "index_to_emoji = [emojis['hat'], emojis['rat'], emojis['cat'], emojis['flat'], emojis['matt'], emojis['cap'], emojis['son']]\n",
    "\n",
    "emoji_encoding = [\n",
    "    [1., 0., 0., 0., 0., 0., 0.],  # 'hat'\n",
    "    [0., 1., 0., 0., 0., 0., 0.], # 'cat'\n",
    "    [0., 0., 1., 0., 0., 0., 0.], # 'rat'\n",
    "    [0., 0., 0., 1., 0., 0., 0.], # 'flat'\n",
    "    [0., 0., 0., 0., 1., 0., 0.],  # 'matt'\n",
    "    [0., 0., 0., 0., 0., 1., 0.],  # 'cap'\n",
    "    [0., 0., 0., 0., 0., 0., 1.]  # 'son'\n",
    "]\n",
    "emoji_encoding_size = len(emoji_encoding)\n",
    "\n",
    "x_train = torch.tensor([hat,\n",
    "                        rat,\n",
    "                        cat,\n",
    "                        flat,\n",
    "                        matt,\n",
    "                        cap,\n",
    "                        son])  # ' All the words '\n",
    "y_train = torch.tensor([hat_y,\n",
    "                        rat_y,\n",
    "                        cat_y,\n",
    "                        flat_y,\n",
    "                        matt_y,\n",
    "                        cap_y,\n",
    "                        son_y])  # ' All the words'\n",
    "\n",
    "test_words = [\"rt\", \"rats\", \"sn\", \"at\", \"cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LongShortTermMemoryModel(nn.Module):\n",
    "    def __init__(self, encoding_size, label_size):\n",
    "        super(LongShortTermMemoryModel, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(char_encoding_size, 128)  # 128 is the state size\n",
    "        self.dense = nn.Linear(128, emoji_encoding_size)  # 128 is the state size\n",
    "\n",
    "    def reset(self, batch_size = 1):  # Reset states prior to new input sequence\n",
    "        zero_state = torch.zeros(1, batch_size, 128)  # Shape: (number of layers, batch size, state size)\n",
    "        self.hidden_state = zero_state\n",
    "        self.cell_state = zero_state\n",
    "\n",
    "    def logits(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        out, (self.hidden_state, self.cell_state) = self.lstm(x, (self.hidden_state, self.cell_state))\n",
    "        return self.dense(out.reshape(-1, 128))\n",
    "\n",
    "    def f(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        return torch.softmax(self.logits(x), dim=1)\n",
    "\n",
    "    def loss(self, x, y):  # x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\n",
    "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 7 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53063/964506695.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_53063/4034948615.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/skole/sem5/machinelearning/test/venv/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 7 is out of bounds."
     ]
    }
   ],
   "source": [
    "model = LongShortTermMemoryModel(char_encoding_size, emoji_encoding_size)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), 0.001)\n",
    "\n",
    "#print(x_train[7])\n",
    "\n",
    "for epoch in range(500):\n",
    "    for i in range(7):\n",
    "        model.reset()\n",
    "        loss = model.loss(x_train[i], y_train[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        # Generate characters from the initial characters ' h'\n",
    "        model.reset()\n",
    "        text = ' h'\n",
    "        model.f(torch.tensor([[char_encodings[0]]]))\n",
    "        y = model.f(torch.tensor([[char_encodings[1]]]))\n",
    "        text += index_to_char[y.argmax(1)]\n",
    "        for c in range(50):\n",
    "            y = model.f(torch.tensor([[char_encodings[y.argmax(1)]]]))\n",
    "            text += index_to_char[y.argmax(1)]\n",
    "        print(text)\n",
    "\n",
    "\n",
    "for word in test_words:\n",
    "    model.reset()\n",
    "    for letter in range(len(word)):\n",
    "        index = index_to_char.index(letter)\n",
    "        emoji_index = model.f(torch.tensor([[char_encodings[index]]]))\n",
    "        if letter == len(word) -1:\n",
    "            print(index_to_emoji[emoji_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
